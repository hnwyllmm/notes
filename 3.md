Aether: A Scalable Approach to Logging

本篇论文讨论的是WAL在多核机器上的并发性能问题。

论文中指出了数据库系统中4个日志相关的影响扩展性的问题：

a. 小I/O请求导致磁盘饱和；

b. 事务在等待日志flush的时候，拿着锁

c. extensive context switching overwhelms(压倒，淹没) the OS scheduler with threads executing log I/Os

d. 事务串行访问内存中的日志数据结构会造成竞争

想要提出一些解决方案，大概有20% ~ 69%的提升。测试的benchmark是TPC-B和TATP。



# 1. Introduction

背景：系统变成多核。大量事务并行运行时，很多事务都是在等待状态。

log manager是数据库的一个核心组件，也是一个瓶颈点，因为它是中心化设计的，并且依赖I/O。日志flush时间、日志引起的锁竞争以及内存中的log buffer的竞争，都会影响扩展性。



## 1.1 Write-Ahead Logging and Log Bottlenecks



![wal-timelines](images/3-1-wal-timelines.png)

几乎所有的数据库系统都设计一个中心化的日志系统，用来保护数据 corruption 和crash之后提交的数据不会丢失。WAL可以让事务修改过的数据页不用写到磁盘。

按照图1描述的，有4个类型的延迟，会影响事务。

** I/O 相关的延迟(A)** 。系统必须保证事务提交之前，log record 到达非易失存储。log flush的时间比较久。log flush还是串行的，多个小请求，会让日志设备负载很高。固态硬盘会好很多。还可以使用一些group commit[8]的手段优化。

** 日志引入的锁竞争(B) **. 所有的写锁都要拿着，直到日志刷新完成。因为锁竞争，系统75%的时间都是空闲的。

** Excessive context switching (C)**. log flush的影响除了IO延迟，还会导致事务等待，需要重新调度。上下文切换和调度会消耗CPU时间，并且不能跟其它工作重叠。多核环境下，硬件上下文的大量切换会导致调度称为瓶颈，如果可执行thread积累的速度比OS能够调度的还要快。

** Log buffer竞争(D)**. 上面的几个瓶颈消除后，就能看到很多线程都在争抢Log buffer。所以可以认为在未来Log buffer是一个最具威胁的扩展瓶颈点。

异步提交可能是最可能消除日志瓶颈的方法。

## 1.2 A Holistic Approach to Scalable Logging

这篇文章介绍Aether，一个完整的日志扩展解决方案，并且证明如何在现代硬件提供方法。
第一，有一个Early Lock Release (ELR),减轻日志引入的锁竞争。ELR在很久之前就提到过多次，但是在主流数据库引擎中并没有使用。ELR可以提高15%~164%的吞吐，尤其是logging to fast flash disk的时候。

第二，Flush Pipelining，允许大部分事务提交时不触发上下文切换。与ELR协同一起，可以跟异步提交一样提升性能但是不损失持久性。

最后，有三个log buffer设计的改善，包括一个新的 consolidation(固结，整合)-based backoff方案，可以让线程们在遇到竞争时聚合它们的请求。

# 2. RELATED WORK
Virtually all database engines employ some variant of
ARIES [14],一个复杂的write-ahead日志系统，集成了支持并发控制的事务回滚和灾难回复，并且允许系统完整的恢复，即使恢复被新的crash重复中断。为了能够达到一个高的鲁棒性，并且拥有良好的性能，ARIES与系统的其它组件紧紧配合，尤其是锁和buffer pool manager，并且对一些访问方法的设计有非常强的影响，比如B+ Tree索引[13]。经典的日志被设计成了一个单一的全局数据结构，由所有的事务共享，使之成为了高并发系统的潜在瓶颈。即使一个单一线程数据库系统，典型的OLTP负载中，日志的大概开销在总时间的12%[7]。

最近一些研究[12][3]评估了固态硬盘在日志中的作用，证明能够巨大提升响应时间和小I/O问题。然而，即使最快的flash磁盘也不能减轻所有的开销，因为日志刷新要求阻塞，因此会引起OS调度。

日志 group commit [8][18]策略通过聚合多个请求，将日志flush聚合成单个I/O操作来减轻日志磁盘的压力。group commit 并不能减少不期望的context switch，因为事务必须阻塞等待日志的通知，而不是阻塞I/O请求。

Asynchronous commit [16][17] 扩展了group commit，不仅仅聚合I/O请求，还允许事务可以不用等待这些请求完成而直接完成。
这个优化完全将log flush的时间从关键路径上移除了，然而牺牲了持久性。如果对应的log record没有持久化并且crash的话，已经提交的数据可能会丢失。尽管不安全，异步提交在商业和开源数据库系统上应用很广泛，因为它提供了很好的性能提升。 相对应的，Aether 达到了这个性能提升，还没有牺牲持久性。

DeWitt et al. [4] 研究出来一个事务可以在日志刷新前安全的释放锁，不过要在特定的条件满足时。 IVS [5] 实现了这个优化但是他的正确性最近才证明[21]. 我们把这个技术当做ELR然后在第3章节做评估。
内存数据库引擎的日志实现是一个特殊的挑战，因为它是唯一的I/O操作。 I/O时间和短事务导致的高并发和log buffer竞争都有影响。现在有些建议是减少日志（还有它的开销）[22]，复制每个事务到多个数据库实例，然后以来hot fail-over来维护持久性。
Aether处理了log flush延迟和log buffer的竞争问题，所以也非常适用于内存数据库。

# 3. MOVING LOG I/O LATENCY OFF THE CRITICAL PATH

事务开始后，日志落盘前，都会一直拿着锁。这样别人不会访问到未提交数据

## 3.1 Early Lock Release
DeWitt et al. [4] 中有一个方法，可以让事务落盘之前，释放掉锁。其它读取之前提交事务的数据的事务，就会依赖它，并且在它的log record落盘之前，不允许返回结果给用户。串行日志实现自然而然的就保留了这个属性，因为依赖的事务的log record必须总要在它之前提交的事务落盘后才能落盘，同样也会在之后持久化(note: 不过还是可以快一点，之前是必须等前一个事务落盘，现在是自己可以先提前读取数据,如果用pipeline角度考虑，还是更快一点)。就像[21]中展示的一样，系统必须符合两个条件来做early lock release，来保持可恢复性（preserve recoverability）:
1. 每个依赖的事务的提交log record都是在对应的之前提交事务的log record之后写入磁盘；
2. 当pre-committed 事务终止，所有的依赖事务也必须终止。大部分系统很自然符合这个条件; 在插入commit record之后除了释放锁就不再工作，因此在恢复的时候只能回滚所有未提交的事务。

ELR通过确保正在提交的事务比如等待它的提交操作完成来将log flush的延迟从关键路径中移除，释放数据库锁，这样其它的事务就可以立即获得这些锁然后继续运行。现代数据库系统并没有实现ELR，并且这篇文章可能是第一个分析经验上的ELR性能的。我们假设这是异步提交[16][17]的作用，已经免去了ELR，并且在几乎所有主流系统中实现了。系统不需要牺牲持久性就可以从ELR中获取很大收益，包括锁竞争和long log flush time。

## 3.2 Evaluation of ELR

我们使用TPC-B[24]来评估ELR。TPC-B是一个数据库压力测试工具并且有严重的锁冲突。Figure 3 shows the benefit of ELR over a baseline
system as we vary the two major factors which impact its effectiveness:
锁竞争和I/O延迟. The y-axis shows
speedup due to ELR as the skew of zipfian-distributed data
accesses increases along the x-axis. 更低的斜率表示更均匀的访问和更少的锁冲突. Different log device
latencies are given as data series ranging from 0 to 10ms. The first series (0ms) is measured using a ramdisk which imposes almost no additional delay beyond a round trip through the OS kernel (40-80μs). The remaining series are created by using a combination of asynchronous I/O and high resolution timers to impose additional response times of 100μs (fast flash drive), 1ms (fast magnetic drive), and 10ms (slow magnetic drive).

如图所示，ELR在低速设备上的提升最高35倍，但是存在竞争的flash设备上是2倍。 This effect occurs because transactions
are short even compared to 100μs I/O times, and ELR eases
contention by removing that delay from the critical path. As write performance of most flash drives remains unpredictable (and usually slower than desired) ELR remains an important optimization even as systems move away from magnetic media.

不同的锁竞争会在三个阶段影响性能。
For very low contention, the probability of a transaction to request
an already-held lock is low. Thus, holding that lock through the log
flush does not stall other transactions and ELR has no opportunity
to improve performance. At the other extreme, very high skew
leads to such high contention that transactions encounter held
locks even with no log flush time. In the middle range, however,
ELR significantly improves performance because holding locks
through log flush causes stalls which would not have arisen otherwise.
The sweet spot becomes wider as longer I/O times stretch out
the total transaction length in the baseline case. Finally, by way of comparison, the intuitive rule that 80% of accesses are to 20% of
the data corresponds roughly to a skew of 0.85. In other words,
workloads are likely to exhibit exactly the contention levels which ELR is well-equipped to reduce.

我们发现即使现代数据库引擎也可以从ELR中获得收益。下一章节会证明，这是一个非常重要的替换异步提交的组件。

# 4. DECOUPLING OS SCHEDULING FROM LOG FLUSH OPERATIONS

log flush的延迟有两个发起点：真正的I/O等待时间和要求等待block和unblock的上下文切换。 已经存在的log flush优化，比如group commit，关注点在I/O等待时间，没有解决线程调度。 Similarly, while ELR
removes log flush from the critical path of other transactions
(shown as (B) in Figure 1) the requesting transaction must still
block for its log flush I/O and be rescheduled as the I/O completes
(shown as (A) in Figure 1). 不像I/O等待时间，OS可以跟其它工作交叉重叠，每个调度角色消耗几微秒CPU时间不能交叉重叠。

调度和上下文切换的开销变得越来越重要，有好几个原因。首先，高性能固态存储提供了十毫秒计的访问时间，使调度变成了总延迟中重要的一个因子。其次，core的指数级增长，让调度的负担也很重，因为每个事务结束OS都需要调度。调度器必须协调所有core上的调度。过度的上下文切换使调度成为瓶颈，表现出来是低CPU利用率和很高的System 时间。

过度的上下文切换解释了为什么仅仅有group commit不是完整的可扩展，并且why asynchronous commit is popular despite being unsafe. 后者减轻了事务提交伴随的上下文切换，然而前者并没有。

## 4.1 Flush Pipelining
为了减轻调度瓶颈（因此提高CPU利用率和吞吐），数据库引擎必须从线程调度中将事务提交分离出来。我们提出了Flush Pipeling，允许代理线程在事务log flush时detach，这样可以做其它事情，在flush完成时重新resume事务。

flush pipeling的操作如下。首先，代理线程异步提交事务（不用等待log flush完成）。然而，不像异步提交，立即返回给客户端，而是脱离事务，将它的状态放到日志中，然后运行其它事务。有一个daemon线程触发log flush，使用类似 group commit的策略(e.g. “每X个事务刷新一次,
L bytes logged, or T time elapsed, whichever comes first”).在每次I/O完成后，daemon通知newly-hardened事务的线程, 重新关联到每个事务, 完成提交过程并且返回结果给客户端。生成log record的事务要终止的话，在rolling back之前也要hardended. 代理线程处理这种场景，相对传统（非乐观）并发控制来说比较少，不用将事务传给daemon。

flush pipeling结合ELR可以提供异步提交一样的吞吐，并且不需要提升任何安全性。只有日志的daemon线程需要等待时间并且因为日志刷新请求而调度，代理线程pipeling多个请求，隐藏了更长的延迟。

## 4.2 Evaluation of Flush Pipelining
flush pipeling成功并且安全地通过断开事务提交与调度的关联，将日志从系统关键执行路径上移除了。

# 5. SCALABLE LOG BUFFER DESIGN

